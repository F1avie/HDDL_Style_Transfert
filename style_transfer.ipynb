{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This neural algorithm renders a content image in the style of another image, achieving so-called style transfer. \n",
    "It relies on a style transfer network which takes a content image $c$ and a style image $s$ as inputs, and synthesizes an output image that recombines the content of the image $c$ with the style of the image $s$. \n",
    "\n",
    "The architecture network is an encoder-decoder with an Adaptive Instance Normalization layer in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data as data\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG-19 as the Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pretrained VGG-19 $f$ is first used to extract the features of the content image $c$ and the style image $s$, namely $f(c)$ and $f(s)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The updated version of VGG19 provided by PyTorch\n",
    "# encoder = torchvision.models.vgg19(weights = \"VGG19_Weights.DEFAULT\", progress = True).features\n",
    "\n",
    "# The encoder available at https://github.com/naoto0804/pytorch-AdaIN\n",
    "encoder = nn.Sequential(\n",
    "    nn.Conv2d(3, 3, (1, 1)),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(3, 64, (3, 3)),\n",
    "    nn.ReLU(),  # relu1-1\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(64, 64, (3, 3)),\n",
    "    nn.ReLU(),  # relu1-2\n",
    "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(64, 128, (3, 3)),\n",
    "    nn.ReLU(),  # relu2-1\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(128, 128, (3, 3)),\n",
    "    nn.ReLU(),  # relu2-2\n",
    "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(128, 256, (3, 3)),\n",
    "    nn.ReLU(),  # relu3-1\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 256, (3, 3)),\n",
    "    nn.ReLU(),  # relu3-2\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 256, (3, 3)),\n",
    "    nn.ReLU(),  # relu3-3\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 256, (3, 3)),\n",
    "    nn.ReLU(),  # relu3-4\n",
    "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 512, (3, 3)),\n",
    "    nn.ReLU(),  # relu4-1, this is the last layer used\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 512, (3, 3)),\n",
    "    nn.ReLU(),  # relu4-2\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 512, (3, 3)),\n",
    "    nn.ReLU(),  # relu4-3\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 512, (3, 3)),\n",
    "    nn.ReLU(),  # relu4-4\n",
    "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 512, (3, 3)),\n",
    "    nn.ReLU(),  # relu5-1\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 512, (3, 3)),\n",
    "    nn.ReLU(),  # relu5-2\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 512, (3, 3)),\n",
    "    nn.ReLU(),  # relu5-3\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 512, (3, 3)),\n",
    "    nn.ReLU()  # relu5-4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Instance Normalization Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Adaptive Instance Normalization is then achieved to align the mean and variance of the content feature maps to those of the style features maps, producing the target feature maps \n",
    "$$t = \\text{AdaIN}(f(c),f(s))=\\sigma(f(s)) \\left( \\frac{f(c)-\\mu(f(c))}{\\sigma(f(c))} \\right) + \\mu(f(c)).$$\n",
    "The mean and variance are computed across spacial dimensions independently for each channel and each sample.\n",
    "\n",
    "More precilely, given ... $x \\in \\mathbb R^{H \\times W}$ (WHAT DOES IT MEAN TO BE COMPLETED cf. INSTANCE NORMALISATION 3.2 car AdaIN calcule mu et sigma de la même façon que IN),\n",
    "$$\\mu_{n c}(x) = \\frac {1}{H W} \\sum_{h=1}^H \\sum_{w=1}^W x_{n c h w},$$\n",
    "$$\\sigma_{n c}(x) = \\sqrt{\\frac {1}{H W} \\sum_{h=1}^H \\sum_{w=1}^W ( x_{n c h w} - \\mu_{n c}(x))^2 + \\epsilon }.$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveIN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, content_feat, style_feat):\n",
    "        return adaptive_instance_normalization(content_feat, style_feat)\n",
    "\n",
    "def adaptive_instance_normalization(self, content_feat, style_feat):\n",
    "    assert (content_feat.size()[:2] == style_feat.size()[:2])\n",
    "    size = content_feat.size()\n",
    "    style_mean, style_std = calc_mean_std(style_feat)\n",
    "    content_mean, content_std = calc_mean_std(content_feat)\n",
    "\n",
    "    normalized_feat = (content_feat - content_mean.expand(\n",
    "        size)) / content_std.expand(size)\n",
    "    return normalized_feat * style_std.expand(size) + style_mean.expand(size)\n",
    "\n",
    "def calc_mean_std(feat, eps=1e-5):\n",
    "    # eps is a small value added to the variance to avoid divide-by-zero.\n",
    "    size = feat.size()\n",
    "    assert (len(size) == 4)\n",
    "    N, C = size[:2]\n",
    "    feat_var = feat.view(N, C, -1).var(dim=2) + eps\n",
    "    feat_std = feat_var.sqrt().view(N, C, 1, 1)\n",
    "    feat_mean = feat.view(N, C, -1).mean(dim=2).view(N, C, 1, 1)\n",
    "    return feat_mean, feat_std\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Decoder $g$, which mostly mirrors the encoder, is finally used to map the target feature map $t$ back to the image space, generating the stylized image $T(c,s) = g(t)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = nn.Sequential(\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 256, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 256, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 256, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 256, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 128, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(128, 128, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(128, 64, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(64, 64, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(64, 3, (3, 3)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style Transfer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-trained VGG-19 is used to compute a loss function to train the decoder ... TO BE CONTINUED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleTransferNet(nn.Module):\n",
    "    def __init__(self, encoder, decoder, style_loss_index = [4, 11, 18, 31]):\n",
    "        super(StyleTransferNet, self).__init__()\n",
    "        # Want to fix the encoder, disable require grad\n",
    "        for _, param in encoder.named_parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        enc_layers = list(encoder.children())\n",
    "        self.enc_1 = nn.Sequential(*enc_layers[:4])   # input -> relu1_1\n",
    "        self.enc_2 = nn.Sequential(*enc_layers[4:11])  # relu1_1 -> relu2_1\n",
    "        self.enc_3 = nn.Sequential(*enc_layers[11:18])  # relu2_1 -> relu3_1\n",
    "        self.enc_4 = nn.Sequential(*enc_layers[18:31])  # relu3_1 -> relu4_1\n",
    "        self.decoder = decoder\n",
    "        self.mse_loss = nn.MSELoss()        \n",
    "\n",
    "    # extract relu1_1, relu2_1, relu3_1, relu4_1 from input image\n",
    "    def encode_with_intermediate(self, input):\n",
    "        # encode the input image and store the intermediate value for \n",
    "        # calculating the final style loss\n",
    "        results = [input]\n",
    "        for i in range(4):\n",
    "            func = getattr(self, 'enc_{:d}'.format(i + 1))\n",
    "            results.append(func(results[-1]))\n",
    "        return results[1:]\n",
    "\n",
    "    # encode the input image to get the features\n",
    "    def encode(self, input):\n",
    "        for i in range(4):\n",
    "            input = getattr(self, 'enc_{:d}'.format(i + 1))(input)\n",
    "        return input\n",
    "\n",
    "    # Content loss: (encoder(decoder(input)) - input)).norm()\n",
    "    def calc_content_loss(self, input, target):\n",
    "        assert (input.size() == target.size())\n",
    "        assert (target.requires_grad is False)\n",
    "        return self.mse_loss(input, target)\n",
    "\n",
    "    # Style loss \n",
    "    def calc_style_loss(self, input, target):\n",
    "        assert (input.size() == target.size())\n",
    "        assert (target.requires_grad is False)\n",
    "        input_mean, input_std = calc_mean_std(input)\n",
    "        target_mean, target_std = calc_mean_std(target)\n",
    "        return self.mse_loss(input_mean, target_mean) + \\\n",
    "               self.mse_loss(input_std, target_std)\n",
    "\n",
    "    def forward(self, content, style, alpha=1.0):\n",
    "        assert 0 <= alpha <= 1    # constant to adjust the compromise between content and style\n",
    "        style_feats = self.encode_with_intermediate(style)   # intermediate values during encoding \n",
    "        content_feat = self.encode(content)  \n",
    "        t = adaptive_instance_normalization(content_feat, style_feats[-1])\n",
    "        t = alpha * t + (1 - alpha) * content_feat\n",
    "\n",
    "        g_t = self.decoder(t)\n",
    "        g_t_feats = self.encode_with_intermediate(g_t)\n",
    "\n",
    "        loss_c = self.calc_content_loss(g_t_feats[-1], t)\n",
    "        loss_s = self.calc_style_loss(g_t_feats[0], style_feats[0])\n",
    "        for i in range(1, 4):\n",
    "            loss_s += self.calc_style_loss(g_t_feats[i], style_feats[i])\n",
    "        return loss_c, loss_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = StyleTransferNet(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transform():\n",
    "    transform_list = [\n",
    "        transforms.Resize(size=(512, 512)),\n",
    "        transforms.RandomCrop(256),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    "    return transforms.Compose(transform_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatFolderDataset(data.Dataset):\n",
    "    def __init__(self, root, transform):\n",
    "        super(FlatFolderDataset, self).__init__()\n",
    "        self.root = root\n",
    "        self.paths = list(Path(self.root).glob('*'))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.paths[index]\n",
    "        img = Image.open(str(path)).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def name(self):\n",
    "        return 'FlatFolderDataset'\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, iteration_count):\n",
    "    \"\"\"Imitating the original implementation\"\"\"\n",
    "    lr = lr / (1.0 + lr_decay * iteration_count)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InfiniteSampler(n):\n",
    "    # i = 0\n",
    "    i = n - 1\n",
    "    order = np.random.permutation(n)\n",
    "    while True:\n",
    "        yield order[i]\n",
    "        i += 1\n",
    "        if i >= n:\n",
    "            np.random.seed()\n",
    "            order = np.random.permutation(n)\n",
    "            i = 0\n",
    "\n",
    "\n",
    "class InfiniteSamplerWrapper(data.sampler.Sampler):\n",
    "    def __init__(self, data_source):\n",
    "        self.num_samples = len(data_source)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(InfiniteSampler(self.num_samples))\n",
    "\n",
    "    def __len__(self):\n",
    "        return 2 ** 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_dir = ...\n",
    "style_dir = ...\n",
    "save_dir = \"./experiments\"\n",
    "lr = 1e-4\n",
    "lr_decay = 5e-5\n",
    "max_iter = 160000\n",
    "batch_size = 8\n",
    "style_weight = 10\n",
    "content_weight = 1\n",
    "n_threads = 16\n",
    "save_model_interval = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = net.decoder\n",
    "vgg = net.vgg\n",
    "\n",
    "vgg.load_state_dict(torch.load(vgg))\n",
    "vgg = nn.Sequential(*list(vgg.children())[:31])\n",
    "network = net.Net(vgg, decoder)\n",
    "network.train()\n",
    "network.to(device)\n",
    "\n",
    "content_tf = train_transform()\n",
    "style_tf = train_transform()\n",
    "\n",
    "content_dataset = FlatFolderDataset(content_dir, content_tf)\n",
    "style_dataset = FlatFolderDataset(style_dir, style_tf)\n",
    "\n",
    "content_iter = iter(data.DataLoader(\n",
    "    content_dataset, batch_size=batch_size,\n",
    "    sampler=InfiniteSamplerWrapper(content_dataset),\n",
    "    num_workers=n_threads))\n",
    "style_iter = iter(data.DataLoader(\n",
    "    style_dataset, batch_size=batch_size,\n",
    "    sampler=InfiniteSamplerWrapper(style_dataset),\n",
    "    num_workers=n_threads))\n",
    "\n",
    "optimizer = torch.optim.Adam(network.decoder.parameters(), lr=lr)\n",
    "\n",
    "loss_content = []\n",
    "loss_style = []\n",
    "for i in tqdm(range(max_iter)):\n",
    "    adjust_learning_rate(optimizer, iteration_count=i)\n",
    "    content_images = next(content_iter).to(device)\n",
    "    style_images = next(style_iter).to(device)\n",
    "    loss_c, loss_s = network(content_images, style_images)\n",
    "    loss_c = content_weight * loss_c\n",
    "    loss_s = style_weight * loss_s\n",
    "    loss = loss_c + loss_s\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_content.append('loss_content', loss_c.item(), i + 1)\n",
    "    loss_style.append('loss_style', loss_s.item(), i + 1)\n",
    "\n",
    "    if (i + 1) % save_model_interval == 0 or (i + 1) == max_iter:\n",
    "        state_dict = net.decoder.state_dict()\n",
    "        for key in state_dict.keys():\n",
    "            state_dict[key] = state_dict[key].to(torch.device('cpu'))\n",
    "        torch.save(state_dict, save_dir /\n",
    "                   'decoder_iter_{:d}.pth.tar'.format(i + 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_transform(size, crop):\n",
    "    transform_list = []\n",
    "    if size != 0:\n",
    "        transform_list.append(transforms.Resize(size))\n",
    "    if crop:\n",
    "        transform_list.append(transforms.CenterCrop(size))\n",
    "    transform_list.append(transforms.ToTensor())\n",
    "    transform = transforms.Compose(transform_list)\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_transfer(vgg, decoder, content, style, alpha=1.0,\n",
    "                   interpolation_weights=None):\n",
    "    assert (0.0 <= alpha <= 1.0)\n",
    "    content_f = vgg(content)\n",
    "    style_f = vgg(style)\n",
    "    if interpolation_weights:\n",
    "        _, C, H, W = content_f.size()\n",
    "        feat = torch.FloatTensor(1, C, H, W).zero_().to(device)\n",
    "        base_feat = adaptive_instance_normalization(content_f, style_f)\n",
    "        for i, w in enumerate(interpolation_weights):\n",
    "            feat = feat + w * base_feat[i:i + 1]\n",
    "        content_f = content_f[0:1]\n",
    "    else:\n",
    "        feat = adaptive_instance_normalization(content_f, style_f)\n",
    "    feat = feat * alpha + content_f * (1 - alpha)\n",
    "    return decoder(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calc_feat_flatten_mean_std(feat):\n",
    "    # takes 3D feat (C, H, W), return mean and std of array within channels\n",
    "    assert (feat.size()[0] == 3)\n",
    "    assert (isinstance(feat, torch.FloatTensor))\n",
    "    feat_flatten = feat.view(3, -1)\n",
    "    mean = feat_flatten.mean(dim=-1, keepdim=True)\n",
    "    std = feat_flatten.std(dim=-1, keepdim=True)\n",
    "    return feat_flatten, mean, std\n",
    "\n",
    "\n",
    "def _mat_sqrt(x):\n",
    "    U, D, V = torch.svd(x)\n",
    "    return torch.mm(torch.mm(U, D.pow(0.5).diag()), V.t())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coral(source, target):\n",
    "    # assume both source and target are 3D array (C, H, W)\n",
    "    # Note: flatten -> f\n",
    "\n",
    "    source_f, source_f_mean, source_f_std = _calc_feat_flatten_mean_std(source)\n",
    "    source_f_norm = (source_f - source_f_mean.expand_as(\n",
    "        source_f)) / source_f_std.expand_as(source_f)\n",
    "    source_f_cov_eye = \\\n",
    "        torch.mm(source_f_norm, source_f_norm.t()) + torch.eye(3)\n",
    "\n",
    "    target_f, target_f_mean, target_f_std = _calc_feat_flatten_mean_std(target)\n",
    "    target_f_norm = (target_f - target_f_mean.expand_as(\n",
    "        target_f)) / target_f_std.expand_as(target_f)\n",
    "    target_f_cov_eye = \\\n",
    "        torch.mm(target_f_norm, target_f_norm.t()) + torch.eye(3)\n",
    "\n",
    "    source_f_norm_transfer = torch.mm(\n",
    "        _mat_sqrt(target_f_cov_eye),\n",
    "        torch.mm(torch.inverse(_mat_sqrt(source_f_cov_eye)),\n",
    "                 source_f_norm)\n",
    "    )\n",
    "\n",
    "    source_f_transfer = source_f_norm_transfer * \\\n",
    "                        target_f_std.expand_as(source_f_norm) + \\\n",
    "                        target_f_mean.expand_as(source_f_norm)\n",
    "\n",
    "    return source_f_transfer.view(source.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "content_paths = \"./input/content/\" + \"flavie_cat.jpg\"\n",
    "style_paths = \"./input/style/\" + \"woman_with_hat_matisse.jpg\"\n",
    "output_dir = \"./output/\"\n",
    "content_size = 512\n",
    "style_size = 512\n",
    "crop = False\n",
    "preserve_color = True\n",
    "alpha = 1\n",
    "save_ext = \".jpg\"\n",
    "interpolation_weights = ''\n",
    "do_interpolation = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'StyleTransferNet' object has no attribute 'vgg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/mhnguyen/5GMM/High-Dimensional-Deep-Learning/Projet/HDDL_Style_Transfert/style_transfer.ipynb Cellule 29\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mhnguyen/5GMM/High-Dimensional-Deep-Learning/Projet/HDDL_Style_Transfert/style_transfer.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m decoder \u001b[39m=\u001b[39m net\u001b[39m.\u001b[39mdecoder\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/mhnguyen/5GMM/High-Dimensional-Deep-Learning/Projet/HDDL_Style_Transfert/style_transfer.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m vgg \u001b[39m=\u001b[39m net\u001b[39m.\u001b[39;49mvgg\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mhnguyen/5GMM/High-Dimensional-Deep-Learning/Projet/HDDL_Style_Transfert/style_transfer.ipynb#X41sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m decoder\u001b[39m.\u001b[39meval()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mhnguyen/5GMM/High-Dimensional-Deep-Learning/Projet/HDDL_Style_Transfert/style_transfer.ipynb#X41sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m vgg\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m~/.conda/envs/deep-learning/lib/python3.9/site-packages/torch/nn/modules/module.py:1207\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1206\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1207\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1208\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'StyleTransferNet' object has no attribute 'vgg'"
     ]
    }
   ],
   "source": [
    "decoder = net.decoder\n",
    "vgg = net.vgg\n",
    "\n",
    "decoder.eval()\n",
    "vgg.eval()\n",
    "\n",
    "decoder.load_state_dict(torch.load(decoder))\n",
    "vgg.load_state_dict(torch.load(vgg))\n",
    "vgg = nn.Sequential(*list(vgg.children())[:31])\n",
    "\n",
    "vgg.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "content_tf = test_transform(content_size, crop)\n",
    "style_tf = test_transform(style_size, crop)\n",
    "\n",
    "for content_path in content_paths:\n",
    "    if do_interpolation:  # one content image, N style image\n",
    "        style = torch.stack([style_tf(Image.open(str(p))) for p in style_paths])\n",
    "        content = content_tf(Image.open(str(content_path))) \\\n",
    "            .unsqueeze(0).expand_as(style)\n",
    "        style = style.to(device)\n",
    "        content = content.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = style_transfer(vgg, decoder, content, style,\n",
    "                                    alpha, interpolation_weights)\n",
    "        output = output.cpu()\n",
    "        output_name = output_dir / '{:s}_interpolation{:s}'.format(\n",
    "            content_path.stem, save_ext)\n",
    "        save_image(output, str(output_name))\n",
    "\n",
    "    else:  # process one content and one style\n",
    "        for style_path in style_paths:\n",
    "            content = content_tf(Image.open(str(content_path)))\n",
    "            style = style_tf(Image.open(str(style_path)))\n",
    "            if preserve_color:\n",
    "                style = coral(style, content)\n",
    "            style = style.to(device).unsqueeze(0)\n",
    "            content = content.to(device).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                output = style_transfer(vgg, decoder, content, style,\n",
    "                                        alpha)\n",
    "            output = output.cpu()\n",
    "\n",
    "            output_name = output_dir / '{:s}_stylized_{:s}{:s}'.format(\n",
    "                content_path.stem, style_path.stem, save_ext)\n",
    "            save_image(output, str(output_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('deep-learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d0f7b78603dc4dbd2d9202bb088bed685f1c0144738bfb4d7d4fba83f0e262d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
